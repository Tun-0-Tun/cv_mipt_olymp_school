{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция №5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Особые точки\n",
    "\n",
    "Настало время перейти к более продвинутым методам сравнения изображений. На прошлых занятих мы рассматривали методы, которые базировались больше на глобальных признаках изображения. Однако, для того, чтобы исследовать схожесть неких изображений, недостаточно иметь представление только глобальных признаков. В качестве шага в определение локальных признаков мы рассматривали метод \"плавающего окна\". \n",
    "\n",
    "Итак, введем предположение, что на изображении можно найти локальные признаки в некой окрестности __ключевой точки__. Вопрос: что брать за ключевую точку и какую окрестность?\n",
    "\n",
    "Для начала разберемся почему глобальные признаки иногда работают плохо на примере следующих изображений.\n",
    "\n",
    "<img src=\"https://i.ibb.co/6FFHWd5/image--000.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "Из всего этого вытекает идея - хочется как-то выделить точки их окрестности, к которым можно привязаться для сравнения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свойства особых точек\n",
    "1. Их должно быть немного\n",
    "    \n",
    "    • существенно меньше, чем пикселей на изображении\n",
    "    \n",
    "    \n",
    "2. Информативные, репрезентативные, уникальные (distinctive)\n",
    "   \n",
    "   • Если окрестности двух точек не отличимы, будет сложно понять, какую из них сопоставить искомому фрагменту\n",
    "   \n",
    "\n",
    "3. Повторяемые (repeatable)\n",
    "    \n",
    "    • Одна и та же точка должна находиться на изображении вне зависимости от геометрических и фотометрических изменений объекта съемки\n",
    "    \n",
    "\n",
    "4. Локальные (local)\n",
    "   \n",
    "   • Небольшого размера, устойчивы к частичному перекрытию другим объектом\n",
    "   \n",
    "### Повторяемость особых точек\n",
    "    ● Необходимо, чтобы хотя бы часть особых точек первого изображения была обнаружена на втором\n",
    "\n",
    "    ● При этом обнаружение особых точек должно происходить независимо для каждого изображения\n",
    "\n",
    "### Информативность, репрезентативность\n",
    "    ● Желательна однозначность в сопоставлении фрагментов\n",
    "    \n",
    "    ● Желательна инвариантность к геометрическим и фотометрическим трансформациям объекта на разных изображениях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Геометрические и фотометрические трансформации изображения.\n",
    "\n",
    "● Геометрические:\n",
    "\n",
    "    ○ Поворот\n",
    "    \n",
    "    ○ Поворот + изменение масштаба\n",
    "    \n",
    "    ○ Афинные преобразования\n",
    "    \n",
    "● Фотометрические\n",
    "\n",
    "    ○ Афинные преобразования интенстивности (I → a I + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение изображений при помощи локальных признаков: основные шаги\n",
    "\n",
    "1. Локализация особых точек\n",
    "\n",
    "\n",
    "2. Выделение особых фрагментов – окрестности ключевых точек, инвариантные к различного рода преобразованиям\n",
    "\n",
    "\n",
    "3. Построение векторов признаков для найденных фрагментов\n",
    "\n",
    "\n",
    "4. Сопоставление наборов локальных признаков для двух изображений\n",
    "    \n",
    "<img src=\"https://i.ibb.co/fD6fRDJ/image--007.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к рассмотрению популярных дексрипторов изображений для особых точек. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris Corner Detection\n",
    "\n",
    "В предыдущей главе мы увидели, что углы - это области изображения с большой разницей в интенсивности во всех направлениях. Одна из ранних попыток найти эти углы была сделана Крисом Харрисом и Майком Стивенсом в их статье «Комбинированный детектор углов и краев» в 1988 году, так что теперь она называется «Детектор углов Харриса». Он перенес эту простую идею в математическую форму. Это в основном находит разницу в интенсивности для смещения (u, v) во всех направлениях. Это выражается как ниже:\n",
    "\n",
    "$$E(u,v) = \\sum_{x,y} \\underbrace{w(x,y)}_\\text{window function} \\, [\\underbrace{I(x+u,y+v)}_\\text{shifted intensity}-\\underbrace{I(x,y)}_\\text{intensity}]^2$$\n",
    "\n",
    "Оконная функция - это либо прямоугольное окно, либо гауссово окно, которое дает вес пикселям внизу.\n",
    "\n",
    "Мы должны максимизировать эту функцию E (u, v) для обнаружения угла. Это означает, что мы должны максимизировать второй срок. Применяя расширение Тейлора к вышеприведенному уравнению и используя некоторые математические шаги (пожалуйста, обратитесь к любым стандартным учебникам, которые вам нравятся для полного вывода), мы получаем окончательное уравнение как:\n",
    "\n",
    "$$E(u,v) \\approx \\begin{bmatrix} u & v \\end{bmatrix} M \\begin{bmatrix} u \\\\ v \\end{bmatrix}$$\n",
    "\n",
    "где\n",
    "\n",
    "$$M = \\sum_{x,y} w(x,y) \\begin{bmatrix}I_x I_x & I_x I_y \\\\\n",
    "                                     I_x I_y & I_y I_y \\end{bmatrix}$$\n",
    "\n",
    "Здесь $I_x$ и $I_y$ являются производными изображения в направлениях $x$ и $y$ соответственно. (Может быть легко обнаружен с помощью cv2.Sobel()).\n",
    "\n",
    "Затем идет основная часть. После этого они создали счет, в основном уравнение, которое определит, может ли окно содержать угол или нет.\n",
    "\n",
    "$$R = det(M) - k(trace(M))^2$$\n",
    "\n",
    "где\n",
    "\n",
    "1. $det (M) = \\lambda_1 \\lambda_2$\n",
    "\n",
    "2. $trace (M) = \\lambda_1 + \\lambda_2$\n",
    "\n",
    "3. $\\lambda_1$ и $\\lambda_2$ - собственные значения $M$\n",
    "\n",
    "Таким образом, значения этих собственных значений определяют, является ли область угловой, кромочной или плоской.\n",
    "\n",
    "1. Когда $|R|$ маленький, что случается, когда $\\lambda_1$ и $\\lambda_2$ маленькие, область плоская.\n",
    "\n",
    "2. Когда $R<0$, что происходит, когда $\\lambda_1 >> \\lambda_2$ или наоборот, область является ребром.\n",
    "\n",
    "3. Когда $R$ большое, что происходит, когда $\\lambda_1$ и $\\lambda_2$ большие и $\\lambda_1 \\sim \\lambda_2$, регион является углом.\n",
    "\n",
    "На картинке это можно представить следующим образом:\n",
    "<img src=\"https://i.ibb.co/3CvVLTP/image--017.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harris Corner Detector in OpenCV\n",
    "Для этой цели в OpenCV есть функция cv2.cornerHarris(). Его аргументы:\n",
    "\n",
    "1. img - входное изображение, оно должно быть в градациях серого и иметь тип float32.\n",
    "2. blockSize - это размер окрестности, рассматриваемый для обнаружения угла\n",
    "3. ksize - параметр апертуры используемого производного Собеля.\n",
    "4. k - свободный параметр детектора Харриса в уравнении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T14:21:10.034663Z",
     "start_time": "2019-07-23T14:21:09.854084Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = 'img/cat.jpg'\n",
    "img = cv2.imread(filename)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T14:15:42.545Z"
    }
   },
   "outputs": [],
   "source": [
    "source_window = 'Source image'\n",
    "corners_window = 'Corners detected'\n",
    "max_thresh = 255\n",
    "def cornerHarris_demo(val):\n",
    "    thresh = val\n",
    "    # Detector parameters\n",
    "    blockSize = 2\n",
    "    apertureSize = 3\n",
    "    k = 0.04\n",
    "    # Detecting corners\n",
    "    dst = cv2.cornerHarris(src_gray, blockSize, apertureSize, k)\n",
    "    # Normalizing\n",
    "    dst_norm = np.empty(dst.shape, dtype=np.float32)\n",
    "    cv2.normalize(dst, dst_norm, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    dst_norm_scaled = cv2.convertScaleAbs(dst_norm)\n",
    "    # Drawing a circle around corners\n",
    "    for i in range(dst_norm.shape[0]):\n",
    "        for j in range(dst_norm.shape[1]):\n",
    "            if int(dst_norm[i,j]) > thresh:\n",
    "                cv2.circle(dst_norm_scaled, (j,i), 5, (0), 2)\n",
    "    # Showing the result\n",
    "    cv2.namedWindow(corners_window)\n",
    "    cv2.imshow(corners_window, dst_norm_scaled)\n",
    "\n",
    "filename = 'img/cat.jpg'\n",
    "src = cv2.imread(filename)\n",
    "if src is None:\n",
    "    print('Could not open or find the image:', args.input)\n",
    "    exit(0)\n",
    "src_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "# Create a window and a trackbar\n",
    "cv2.namedWindow(source_window)\n",
    "thresh = 200 # initial threshold\n",
    "cv2.createTrackbar('Threshold: ', source_window, thresh, max_thresh, cornerHarris_demo)\n",
    "cv2.imshow(source_window, src)\n",
    "cornerHarris_demo(thresh)\n",
    "if cv2.waitKey() == ord('q'):\n",
    "    pass\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shi-Tomasi Corner Detector & Good Features to Track\n",
    "В прошлой главе мы увидели детектор углов Харриса. Позже в 1994 году Дж. Ши и К. Томази внесли небольшую модификацию в свою статью Good Features to Track, которая показывает лучшие результаты по сравнению с детектором Harris Corner Detector. Оценочную функцию в Harris Corner Detector предоставили:\n",
    "\n",
    "$$R = \\lambda_1 \\lambda_2 - k(\\lambda_1+\\lambda_2)^2$$\n",
    "\n",
    "Вместо этого Ши-Томаси предложил:\n",
    "\n",
    "$$R = min(\\lambda_1, \\lambda_2)$$\n",
    "\n",
    "Если оно больше порогового значения, оно рассматривается как угол. Если мы построим его в пространстве $\\lambda_1 - \\lambda_2$, как мы это делали в Harris Corner Detector, мы получим изображение, как показано ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/sCBQZVp/shitomasi_space.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "\n",
    "Из рисунка видно, что только когда $\\lambda_1$ и $\\lambda_2$ превышают минимальное значение $\\lambda_ {min}$, оно считается углом (зеленая область)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shi-Tomasi Corner Detector in OpenCV\n",
    "\n",
    "OpenCV имеет функцию cv2.goodFeaturesToTrack (). Он находит N самых сильных углов на изображении методом Ши-Томази (или Обнаружение углов Харриса, если вы его укажете). Как обычно, изображение должно быть изображением в градациях серого. Затем вы указываете количество углов, которые вы хотите найти. Затем вы указываете уровень качества, который является значением в диапазоне 0-1, что обозначает минимальное качество угла, ниже которого все отклоняются. Затем мы предоставляем минимальное евклидово расстояние между обнаруженными углами.\n",
    "\n",
    "Со всей этой информацией функция находит углы на изображении. Все углы ниже уровня качества отклоняются. Затем сортирует оставшиеся углы по качеству в порядке убывания. Затем функция занимает первый самый сильный угол, отбрасывает все близлежащие углы в диапазоне минимального расстояния и возвращает N самых сильных углов.\n",
    "\n",
    "В приведенном ниже примере мы попытаемся найти 25 лучших углов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-23T14:16:13.306Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/cat.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "corners = cv2.goodFeaturesToTrack(gray,25,0.01,10)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x,y = i.ravel()\n",
    "    cv2.circle(img,(x,y),3,255,-1)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rng\n",
    "\n",
    "source_window = 'Image'\n",
    "maxTrackbar = 100\n",
    "rng.seed(12345)\n",
    "def goodFeaturesToTrack_Demo(val):\n",
    "    maxCorners = max(val, 1)\n",
    "    # Parameters for Shi-Tomasi algorithm\n",
    "    qualityLevel = 0.01\n",
    "    minDistance = 10\n",
    "    blockSize = 3\n",
    "    gradientSize = 3\n",
    "    useHarrisDetector = False\n",
    "    k = 0.04\n",
    "    # Copy the source image\n",
    "    copy = np.copy(src)\n",
    "    # Apply corner detection\n",
    "    corners = cv2.goodFeaturesToTrack(src_gray, maxCorners, qualityLevel, minDistance, None, \\\n",
    "        blockSize=blockSize, gradientSize=gradientSize, useHarrisDetector=useHarrisDetector, k=k)\n",
    "    # Draw corners detected\n",
    "#     print('** Number of corners detected:', corners.shape[0])\n",
    "    radius = 4\n",
    "    for i in range(corners.shape[0]):\n",
    "        cv2.circle(copy, \n",
    "                   (corners[i,0,0], corners[i,0,1]), radius, (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256)),\n",
    "                   cv2.FILLED)\n",
    "    # Show what you got\n",
    "    cv2.namedWindow(source_window)\n",
    "    cv2.imshow(source_window, copy)\n",
    "    \n",
    "# Load source image and convert it to gray\n",
    "\n",
    "filename = 'img/cat.jpg'\n",
    "src = cv2.imread(filename)\n",
    "if src is None:\n",
    "    print('Could not open or find the image:', args.input)\n",
    "    exit(0)\n",
    "src_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "# Create a window and a trackbar\n",
    "cv2.namedWindow(source_window)\n",
    "maxCorners = 23 # initial threshold\n",
    "cv2.createTrackbar('Threshold: ', source_window, maxCorners, maxTrackbar, goodFeaturesToTrack_Demo)\n",
    "cv2.imshow(source_window, src)\n",
    "goodFeaturesToTrack_Demo(maxCorners)\n",
    "if cv2.waitKey() == ord('q'):\n",
    "    pass\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SIFT (Scale-Invariant Feature Transform)\n",
    "\n",
    "В последних двух главах мы видели некоторые детекторы углов, такие как Харрис и т. Д. Они не зависят от вращения, что означает, что даже если изображение поворачивается, мы можем найти те же углы. Это очевидно, потому что углы остаются и в повернутом изображении. Но как насчет масштабирования? Угол может не быть углом, если изображение масштабировано. Например, проверьте простое изображение ниже. Угол на небольшом изображении в небольшом окне является плоским, когда он увеличен в том же окне. Так что угол Харриса не является масштабно-инвариантным.\n",
    "\n",
    "<img src=\"https://i.ibb.co/N7kR8Lc/sift_scale_invariant.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "Рассмотрим новый алгоритм Scale Invariant Feature Transform (SIFT), который позволяет сохранить инвариантость к масштабированию.\n",
    "\n",
    "### 1. Обнаружение экстремумов в пространстве\n",
    "\n",
    "Из рисунка выше видно, что мы не можем использовать одно и то же окно для обнаружения ключевых точек с разным масштабом. Это нормально с небольшим углом. Но чтобы обнаружить большие углы, нам нужны большие окна. Для этого используется масштабная фильтрация. В нем найден лапласиан гауссовский для изображения с различными значениями $\\sigma$. LoG действует как детектор капель, который обнаруживает капли разных размеров из-за изменения $\\sigma$. Короче говоря, $\\sigma$ действует как параметр масштабирования. Например, на изображении выше, ядро Гаусса с низким значением $\\sigma$ дает высокое значение для небольшого угла, в то время как ядро гауссиана с высоким значением $\\sigma$ хорошо подходит для большего угла. Таким образом, мы можем найти локальные максимумы по шкале и пространству, которые дают нам список значений $(x, y, \\sigma)$, что означает, что существует потенциальная ключевая точка в точке $(x, y)$ в масштабе $\\sigma$.\n",
    "\n",
    "Но этот LoG немного дорог, поэтому алгоритм SIFT использует разность гауссианов, которая является приближением LoG. Разность по Гауссу получается как разница по размытию по Гауссу изображения с двумя разными $\\sigma$, пусть это будут $\\sigma$ и $k \\sigma$. Этот процесс выполняется для разных октав изображения в гауссовой пирамиде. Это представлено на изображении ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/WWyXGrD/sift_dog.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Как только эта DoG найдена, изображения ищут локальные экстремумы в масштабе и пространстве. Например, один пиксель в изображении сравнивается с его 8 соседями, а также с 9 пикселями в следующем масштабе и 9 пикселями в предыдущих масштабах. Если это локальный экстремум, это потенциальная ключевая точка. Это в основном означает, что ключевой момент лучше всего представлен в этом масштабе. Это показано на рисунке ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/2YN6FZ6/sift_local_extrema.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Что касается различных параметров, в статье приведены некоторые эмпирические данные, которые можно суммировать как: число $octaves = 4$, количество уровней $scale = 5$, начальные $\\sigma = 1.6$, $k = \\sqrt {2}$ и т. Д. В качестве оптимальных значений.\n",
    "\n",
    "### 2. Локализация ключевых точек\n",
    "\n",
    "Как только потенциальные ключевые точки найдены, их необходимо уточнить, чтобы получить более точные результаты. Они использовали расширение масштаба шкалы Тейлора, чтобы получить более точное местоположение экстремумов, и если интенсивность этих экстремумов меньше порогового значения (0,03 согласно статье), оно отклоняется. Этот порог называется контрастным порогом в OpenCV\n",
    "\n",
    "У DoG более высокий отклик на ребра, поэтому ребра также должны быть удалены. Для этого используется концепция, похожая на угловой детектор Harris. Они использовали матрицу Гессиана 2x2 (H) для вычисления кривизны кривизны. Из углового детектора Харриса мы знаем, что для ребер одно собственное значение больше другого. Так что здесь они использовали простую функцию,\n",
    "\n",
    "Если это отношение больше порога, называемого edgeThreshold в OpenCV, эта ключевая точка отбрасывается. Это дано как 10 в статье.\n",
    "\n",
    "Таким образом, он устраняет любые малоконтрастные ключевые точки и граничные ключевые точки, а остающиеся точки - это сильные точки интереса.\n",
    "\n",
    "### 3. Ориентация значения\n",
    "\n",
    "Теперь ориентация назначается каждой ключевой точке для достижения неизменности вращения изображения. Вокруг местоположения ключевой точки берется окрестность в зависимости от масштаба, и в этой области вычисляются величина и направление градиента. Создается гистограмма ориентации с 36 ячейками, охватывающими 360 градусов. (Он взвешивается по величине градиента и по гауссовому круглому окну с $\\sigma$, равным 1,5-кратному масштабу ключевой точки. Принимается самый высокий пик в гистограмме, и любой пик выше 80% также считается для расчета ориентации. Он создает ключевые точки с одинаковым расположением и масштабом, но разными направлениями, что способствует стабильности сопоставления.\n",
    "\n",
    "### 4. Дескриптор ключевой точки\n",
    "\n",
    "Теперь дескриптор ключевой точки создан. Окрестность 16x16 вокруг ключевой точки берется. Он разделен на 16 субблоков размером 4х4. Для каждого субблока создается гистограмма ориентации 8 бинов. Таким образом, доступно 128 значений бина. Он представлен в виде вектора для формирования дескриптора ключевой точки. В дополнение к этому, предприняты некоторые меры для достижения устойчивости к изменениям освещенности, вращению и т. Д.\n",
    "\n",
    "### 5. Сопоставление ключевых точек \n",
    "\n",
    "Ключевые точки между двумя изображениями сопоставляются путем определения ближайших соседей. Но в некоторых случаях второй ближайший матч может быть очень близко к первому. Это может произойти из-за шума или по другим причинам. В этом случае берется отношение ближайшего расстояния ко второму ближайшему расстоянию. Если оно больше 0,8, они отклоняются. Это устраняет около 90% ложных совпадений, в то время как отбрасывает только 5% правильных совпадений, согласно статье.\n",
    "\n",
    "Так что это краткое изложение алгоритма SIFT. Для получения более подробной информации и понимания, чтение оригинальной статьи настоятельно рекомендуется. Помните одну вещь, этот алгоритм запатентован. Таким образом, этот алгоритм включен в модуль Non-free в OpenCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T14:23:23.944534Z",
     "start_time": "2019-07-23T14:23:21.791130Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'img/cat.jpg'\n",
    "src = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "if src is None:\n",
    "    print('Could not open or find the image:', args.input)\n",
    "    exit(0)\n",
    "#-- Step 1: Detect the keypoints using SURF Detector\n",
    "minHessian = 600\n",
    "detector = cv2.xfeatures2d_SURF.create(hessianThreshold=minHessian)\n",
    "keypoints = detector.detect(src)\n",
    "#-- Draw keypoints\n",
    "img_keypoints = np.empty((src.shape[0], src.shape[1], 3), dtype=np.uint8)\n",
    "cv2.drawKeypoints(src, keypoints, img_keypoints)\n",
    "#-- Show detected (drawn) keypoints\n",
    "cv2.imshow('SURF Keypoints', img_keypoints)\n",
    "if cv2.waitKey() == ord('q'):\n",
    "    pass\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SURF (Speeded-Up Robust Features)\n",
    "\n",
    "В прошлой главе мы увидели SIFT для обнаружения и описания ключевых точек. Но это было сравнительно медленно, и людям нужна была более ускоренная версия. В 2006 году три человека, Бей, Х., Туйтелаарс, Т. и Ван Гул, Л., опубликовали еще одну статью «SURF: Ускоренные надежные функции», в которой был представлен новый алгоритм под названием SURF. Как следует из названия, это ускоренная версия SIFT.\n",
    "\n",
    "В SIFT Лоу аппроксимировал лапласиан гауссиана с разностью гауссов для нахождения масштаба пространства. SURF идет немного дальше и приближается к LoG с Box Filter. На рисунке ниже показана демонстрация такого приближения. Одним из больших преимуществ этого приближения является то, что свертка с блочным фильтром может быть легко рассчитана с помощью интегральных изображений. И это можно сделать параллельно для разных масштабов. Также SURF полагаются на определитель матрицы Гессена как для масштаба, так и для местоположения.\n",
    "\n",
    "<img src=\"https://i.ibb.co/hgJT5rC/surf_boxfilter.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Для присвоения ориентации SURF использует вейвлет-отклики в горизонтальном и вертикальном направлении для окрестности. Применяем размытие по гауссу. Затем они наносятся на график, как показано на рисунке ниже. Доминирующая ориентация оценивается путем вычисления суммы всех откликов в скользящем окне ориентации под углом 60 градусов. Интересно то, что вейвлет-отклик можно легко определить с помощью интегральных изображений в любом масштабе. Для многих применений инвариантность вращения не требуется, поэтому нет необходимости находить эту ориентацию, что ускоряет процесс. SURF предоставляет такую функциональность, которая называется Upright-SURF или U-SURF. Это улучшает скорость и работает до $\\pm 15^{\\circ}$. OpenCV поддерживает оба, в зависимости от флага, в вертикальном положении. Если это 0, ориентация рассчитывается. Если это 1, ориентация не рассчитывается, и это быстрее.\n",
    "\n",
    "<img src=\"https://i.ibb.co/ys4gc6q/surf_orientation.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Для описания функции SURF использует отклики вейвлета в горизонтальном и вертикальном направлении (опять же, использование встроенных изображений облегчает задачу). Вокруг ключевой точки, где s - размер, берется окрестность размером 20x20. Он разделен на 4x4 субрегиона. Для каждого субрегиона принимаются горизонтальные и вертикальные вейвлет-отклики, и вектор формируется следующим образом: $v = (\\sum{d_x}, \\sum{d_y}, \\sum{| d_x |}, \\sum{|d_y|})$ , Это, когда представлено как вектор, дает дескриптор функции SURF с общим количеством 64 измерений. Чем меньше размер, тем выше скорость вычислений и сопоставлений, но обеспечьте лучшую различимость функций.\n",
    "\n",
    "Для большей самобытности дескриптор функции SURF имеет расширенную версию 128 измерений. Суммы $d_x$ и $|d_x|$ вычисляются отдельно для $d_y < 0$ и $d_y \\geq 0$. Аналогично, суммы $d_y$ и $|d_y|$ делятся по знаку $d_x$, удваивая количество признаков. Это не добавляет больших вычислительных сложностей. OpenCV поддерживает оба, устанавливая значение флага, расширенного с 0 и 1 для 64-dim и 128-dim соответственно (по умолчанию 128-dim)\n",
    "\n",
    "Другим важным улучшением является использование знака Лапласа (следа матрицы Гессе) для основной точки интереса. Это не добавляет затрат на вычисление, поскольку оно уже вычислено во время обнаружения. Знак лапласиана отличает яркие пятна на темном фоне от обратной ситуации. На этапе сопоставления мы сравниваем объекты только в том случае, если они имеют одинаковый тип контраста (как показано на рисунке ниже). Эта минимальная информация обеспечивает более быстрое сопоставление без снижения производительности дескриптора.\n",
    "\n",
    "<img src=\"https://i.ibb.co/WzkxPhd/surf_matching.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Короче говоря, SURF добавляет множество функций для улучшения скорости на каждом этапе. Анализ показывает, что он в 3 раза быстрее, чем SIFT, а производительность сравнима с SIFT. SURF хорош для обработки изображений с размытостью и поворотом, но не справляется с изменением точек обзора и освещенности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURF in OpenCV\n",
    "\n",
    "OpenCV обеспечивает функциональность SURF так же, как SIFT. Вы инициируете объект SURF с некоторыми дополнительными условиями, такими как дескрипторы 64/128-dim, вертикальный / нормальный SURF и т. Д. Все детали хорошо объяснены в документах. Затем, как мы это делали в SIFT, мы можем использовать SURF.detect(), SURF.compute() и т. Д. Для поиска ключевых точек и дескрипторов.\n",
    "\n",
    "Сначала мы увидим простую демонстрацию того, как найти ключевые точки и дескрипторы SURF и нарисовать их. Все примеры показаны в терминале Python, поскольку он такой же, как и в SIFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T14:45:26.015238Z",
     "start_time": "2019-07-23T14:45:25.774981Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input1 = 'img/asd.png'\n",
    "img = cv2.imread(input1, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "minHessian = 1000\n",
    "detector = cv2.xfeatures2d_SURF.create(hessianThreshold=minHessian)\n",
    "keypoints = detector.detect(img)\n",
    "\n",
    "img_keypoints = np.empty((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "cv2.drawKeypoints(img, keypoints, img_keypoints)\n",
    "#-- Show detected (drawn) keypoints\n",
    "plt.imshow(img_keypoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T14:45:36.750736Z",
     "start_time": "2019-07-23T14:45:33.953713Z"
    }
   },
   "outputs": [],
   "source": [
    "input1 = 'img/asd.png'\n",
    "input2 = 'img/fhj.png'\n",
    "img1 = cv2.imread(input1, cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(input2, cv2.IMREAD_GRAYSCALE)\n",
    "if img1 is None or img2 is None:\n",
    "    print('Could not open or find the images!')\n",
    "    exit(0)\n",
    "#-- Step 1: Detect the keypoints using SURF Detector, compute the descriptors\n",
    "minHessian = 5000\n",
    "detector = cv2.xfeatures2d_SURF.create(hessianThreshold=minHessian)\n",
    "keypoints1, descriptors1 = detector.detectAndCompute(img1, None)\n",
    "keypoints2, descriptors2 = detector.detectAndCompute(img2, None)\n",
    "#-- Step 2: Matching descriptor vectors with a brute force matcher\n",
    "# Since SURF is a floating-point descriptor NORM_L2 is used\n",
    "matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE)\n",
    "matches = matcher.match(descriptors1, descriptors2)\n",
    "#-- Draw matches\n",
    "img_matches = np.empty((max(img1.shape[0], img2.shape[0]), img1.shape[1]+img2.shape[1], 3), dtype=np.uint8)\n",
    "cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches, img_matches)\n",
    "#-- Show detected matches\n",
    "cv2.imshow('Matches', img_matches)\n",
    "if cv2.waitKey() == ord('q'):\n",
    "    pass\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
