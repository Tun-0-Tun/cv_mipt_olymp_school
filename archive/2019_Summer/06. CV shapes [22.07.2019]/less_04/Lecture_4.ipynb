{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция №4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Производная в виде свертки\n",
    "\n",
    "На прошлом лекции, мы рассмотрели операцию свёртки и отметили, что свёртка — это очень полезная и распространённая операция, лежащая в основе различных фильтров. \n",
    "\n",
    "Одна из важнейших свёрток $-$ это вычисление производных. \n",
    "В математике и физике производные играют очень важную роль, то же самое можно сказать и про компьютерное зрение.\n",
    "Но что же это за производная от изображения? Как мы помним, изображения, с которыми мы работаем, состоят из пикселей, которые, для картинки в градациях серого, задают значение яркости.\n",
    "Т.е. наша картинка $-$ это просто двумерная матрица чисел. Теперь вспомним, что же такое производная.\n",
    "***\n",
    "**Производная (функции в точке)** $-$ это скорость изменения функции (в данной точке). Определяется как предел отношения приращения функции к приращению ее аргумента при стремлении приращения аргумента к нулю.\n",
    "***\n",
    "\n",
    "Получается, что, в нашем случае, производная — это отношение значения приращения пикселя по y к значению приращению пикселя по x: $dI = \\frac{dy}{dx}$\n",
    "\n",
    "Работая с изображением I, мы работает с функцией двух переменных $I(x,y)$, т.е. со скалярным полем. Поэтому, более правильно говорить не о производной, а о градиенте изображения.\n",
    "***\n",
    "**Градиент (от лат. gradiens — шагающий, растущий)** $-$ вектор, показывающий направление наискорейшего возрастания некоторой величины, значение которой меняется от одной точки пространства к другой (скалярного поля).\n",
    "***\n",
    "\n",
    "Если каждой точке M области многомерного пространства поставлено в соответствие некоторое (обычно $-$ действительное) число $u$, то говорят, что в этой области задано скалярное поле.\n",
    "\n",
    "\n",
    "Итак, градиент для каждой точки изображения (функция яркости) — двумерный вектор, компонентами которого являются производные яркости изображения по горизонтали и вертикали $-$ $grad[I(x,y)] = (\\frac{dI}{dx}, \\frac{dI}{dy})$\n",
    "\n",
    "В каждой точке изображения градиентный вектор ориентирован в направлении наибольшего увеличения яркости, а его длина соответствует величине изменения яркости.\n",
    "\n",
    "вектор (в заданной точке) задаётся двумя значениями: длиной и направлением.\n",
    "\n",
    "* длинной: $\\sqrt{dx^2 + dy^2}$;\n",
    "\n",
    "* направление $-$ угол между вектором и осью $x$: $\\tan^{-1}(\\frac{dy}{dx})$;\n",
    "\n",
    "\n",
    "Для дифференцирования изображения используется, так называемый, оператор **Собеля**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оператор Собеля** $-$ это дискретный дифференциальный оператор, вычисляющий приближение градиента яркости изображения.\n",
    "Оператор вычисляет градиент яркости изображения в каждой точке. Так находится направление наибольшего увеличения яркости и величина её изменения в этом направлении. Результат показывает, насколько «резко» или «плавно» меняется яркость изображения в каждой точке, а значит, вероятность нахождения точки на грани, а также ориентацию границы.\n",
    "\n",
    "Т.о. результатом работы оператора Собеля в точке области постоянной яркости будет нулевой вектор, а в точке, лежащей на границе областей различной яркости — вектор, пересекающий границу в направлении увеличения яркости.\n",
    "\n",
    "Наиболее часто оператор Собеля применяется в алгоритмах выделения границ. \n",
    "\n",
    "Оператор Собеля основан на свёртке изображения небольшими целочисленными фильтрами в вертикальном и горизонтальном направлениях, поэтому его относительно легко вычислять. Оператор использует ядра 3x3, с которыми свёртывают исходное изображение для вычисления приближенных значений производных по горизонтали и по вертикали.\n",
    "\n",
    "\n",
    "### Формализация\n",
    "Пусть ${\\displaystyle \\mathbf {A} }$ $-$ это исходное изображение, а ${\\displaystyle \\mathbf {G} _{x}}$ и ${\\displaystyle \\mathbf {G} _{y}}$ $-$ два изображения, на которых каждая точка содержит приближённые производные по ${\\displaystyle x}$ и по ${\\displaystyle y}$. Они вычисляются следующим образом:\n",
    "\n",
    "${\\displaystyle \\mathbf {G} _{y}={\\begin{bmatrix}-1&-2&-1\\\\0&0&0\\\\+1&+2&+1\\\\\\end{bmatrix}}*\\mathbf {A} \\quad {\\mbox{and}}\\quad \\mathbf {G} _{x}={\\begin{bmatrix}-1&0&+1\\\\-2&0&+2\\\\-1&0&+1\\end{bmatrix}}*\\mathbf {A} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контур и как его найти"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Контурный анализ** $-$ это один из важных и очень полезных методов описания, хранения, распознавания, сравнения и поиска графических образов/объектов. \n",
    "\n",
    "**Контур** $-$ это внешние очертания (обвод) предмета/объекта.\n",
    "\n",
    "При проведении контурного анализа:\n",
    "* полагается, что контур содержит достаточную информацию о форме объекта;\n",
    "* внутренние точки объекта во внимание не принимаются. \n",
    "\n",
    "Вышеприведённые положения, разумеется, накладывают существенные ограничения на область применения контурного анализа, которые, в основном, связаны с проблемами выделения контура на изображениях:\n",
    "* из-за одинаковой яркости с фоном объект может не иметь чёткой границы, или может быть зашумлён помехами, что приводит к невозможности выделения контура;\n",
    "* перекрытие объектов или их группировка приводит к тому, что контур выделяется неправильно и не соответствует границе объекта.\n",
    "\n",
    "Однако, переход к рассмотрению только контуров объектов позволяет уйти от пространства изображения – к пространству контуров, что существенно снижает сложность алгоритмов и вычислений. \n",
    "\n",
    "Т.о., контурный анализ имеет довольно слабую устойчивость к помехам, и любое пересечение или лишь частичная видимость объекта приводит либо к невозможности детектирования, либо к ложным срабатываниям, но простота и быстродействие контурного анализа, позволяют вполне успешно применять данный подход (при чётко выраженном объекте на контрастном фоне и отсутствии помех).\n",
    "\n",
    "Итак, мы определились, что контур — это некая граница объекта, которая отделяет его от фона (других объектов). \n",
    "\n",
    "Во всех случаях мы получаем бинарное изображение, которое явным образом задаёт нам границы объекта. Вот эта совокупность пикселей, составляющих границу объекта и есть контур объекта.\n",
    "\n",
    "Чтобы оперировать полученным контуром, его необходимо как-то представить (закодировать). \n",
    "Например, указывать вершины отрезков, составляющих контур.\n",
    "Другой известный способ кодирования контура $-$ это **цепной код Фримена**. Этот метод будет рассмотрен чуть позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оператор Лапласа\n",
    "Он вычисляет лапласиан изображения, заданного соотношением,\n",
    "${\\Delta src = \\frac{\\partial ^2{src}}{\\partial x^2} + \\frac{\\partial ^2{src}}{\\partial y^2}}$\n",
    "где каждая производная находится с использованием производных Собеля. Если ksize = $1$, то для фильтрации используется следующее ядро:\n",
    "\n",
    "$$\n",
    "{K = \\begin{pmatrix}\n",
    "0 & \\ 1 & \\ 0 \\\\ \n",
    "1 & \\ -4 & \\ 1 \\\\ \n",
    "0 & \\ 1 & \\ 0 \n",
    "\\end{pmatrix}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T20:31:53.364183Z",
     "start_time": "2019-07-16T20:31:52.610814Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/road_2.jpg')\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "## выделяем границы\n",
    "laplac = cv2.Laplacian(gray_img, cv2.THRESH_BINARY, scale=0.25, ksize=5)\n",
    "laplac = cv2.medianBlur(laplac, 5)\n",
    "\n",
    "fig, m_axs = plt.subplots(1,2, figsize=(14,12))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('gray')\n",
    "ax1.imshow(gray_img, cmap='gray')\n",
    "ax2.set_title('laplaccian')\n",
    "ax2.imshow(laplac, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектор границ Кенни (Canny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория\n",
    "**Canny Edge Detection** $-$ популярный алгоритм обнаружения краев. Это многоступенчатый алгоритм, и мы пройдем через все этапы.\n",
    "\n",
    "1. **Шумоподавление**\n",
    "\n",
    "Поскольку обнаружение краев подвержено воздействию шума на изображении, первым шагом является удаление шума на изображении с помощью фильтра Гаусса $5\\times5$. Мы уже видели это в предыдущих главах.\n",
    "\n",
    "2. **Поиск градиента интенсивности изображения**\n",
    "\n",
    "Затем сглаженное изображение фильтруется ядром Собеля (рассмотрен выше) в горизонтальном и вертикальном направлении, чтобы получить первую производную в горизонтальном направлении ($G_x$) и вертикальном направлении ($G_y$). Из этих двух изображений мы можем найти градиент края и направление для каждого пикселя следующим образом:\n",
    "\n",
    "$${Edge(G) = \\sqrt{ G_x^2 + G_y^2}}$$\n",
    "$${Angle(\\theta) = \\tan^{-1}(\\frac{G_x}{G_y})}$$\n",
    "\n",
    "Направление градиента всегда перпендикулярно краям. Он округлен до одного из четырех углов, представляющих вертикальное, горизонтальное и два диагональных направления.\n",
    "\n",
    "3. **Немаксимальное подавление**\n",
    "\n",
    "После получения величины и направления градиента выполняется полное сканирование изображения для удаления любых нежелательных пикселей, которые могут не составлять края. Для этого в каждом пикселе пиксель проверяется, является ли он локальным максимумом в его окрестности в направлении градиента. Проверьте изображение ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/XZZVNmK/nms.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "Точка $А$ находится на краю (в вертикальном направлении). Направление градиента нормальное к краю. Точки $B$ и $C$ находятся в градиентных направлениях. Таким образом, точка $A$ проверяется с помощью точек $B$ и $C$, чтобы увидеть, образует ли она локальный максимум. Если это так, он рассматривается для следующего этапа, в противном случае он подавляется (обнуляется).\n",
    "\n",
    "Короче говоря, в результате вы получите бинарное изображение с «тонкими краями».\n",
    "\n",
    "4. **Гистерезис пороговый**\n",
    "\n",
    "Эта стадия решает, какие ребра действительно являются ребрами, а какие нет. Для этого нам понадобятся два пороговых значения, **minVal** и **maxVal**. Любые ребра с градиентом интенсивности, превышающим **maxVal**, обязательно будут ребрами, а ребра ниже **minVal** не будут ребрами, поэтому отбрасываются. Те, кто лежит между этими двумя порогами, классифицируются как ребра или не ребра в зависимости от их связности. Если они связаны с точными пикселями, они считаются частью ребер. В противном случае они также отбрасываются. Смотрите изображение ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/cryRqvD/hysteresis.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "Край $A$ выше **maxVal**, так что считается «верным краем». Хотя ребро $C$ меньше **maxVal**, оно связано с ребром $A$, так что это также считается допустимым ребром, и мы получаем эту полную кривую. Но ребро $B$, хотя оно выше **minVal** и находится в той же области, что и ребро $C$, не связано с каким-либо «верным краем», поэтому отбрасывается. Поэтому очень важно, чтобы мы выбрали соответственно **minVal** и **maxVal**, чтобы получить правильный результат.\n",
    "\n",
    "На этом этапе также удаляются небольшие пиксельные шумы в предположении, что края являются длинными линиями.\n",
    "\n",
    "Итак, что мы в итоге получаем, это сильные края изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектор границ Кенни в OpenCV\n",
    "OpenCV помещает все вышеперечисленное в одну функцию **cv2.Canny(image,threshold1,threshold2,apertureSize,L2gradient)**. \n",
    "\n",
    "* **image** $-$ это наше входное изображение\n",
    "* **threshold1** $-$ minVal для процедуры гистерезиса\n",
    "* **threshold2** $-$ maxVal для процедуры гистерезиса\n",
    "* **apertureSize** $-$ размер ядра Собеля, используемый для поиска градиентов изображения, по умолчанию равен $3$\n",
    "* **L2gradient** $-$ флаг, определяет уравнение для определения величины градиента. Если это True, он использует упомянутое выше уравнение, которое является более точным, в противном случае он использует эту функцию: Edge_Gradient($G$) = |$G_x$| + |$G_y$|. По умолчанию это False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T20:32:02.233842Z",
     "start_time": "2019-07-16T20:32:01.554873Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/road_2.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "#sobel = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5)\n",
    "edges = cv2.Canny(img,360,360,5)\n",
    "\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Edge Image')\n",
    "ax2.imshow(edges, cmap='gray');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контуры\n",
    "\n",
    "Полученные границы достаточно просто преобразуются в контуры. Для алгоритма Кэнни это происходит автоматически, для остальных алгоритмов требуется дополнительная бинаризация. Получить контур для бинарного алгоритма можно например алгоритмом [жука](http://wiki.technicalvision.ru/index.php/%D0%92%D1%8B%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B8_%D0%BE%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%D0%B8%D0%B5_%D0%BA%D0%BE%D0%BD%D1%82%D1%83%D1%80%D0%BE%D0%B2).\n",
    "\n",
    "В OpenCV поиск контуров похож на поиск белого объекта на черном фоне. Помните, что объект, который нужно найти, должен быть белым, а фон должен быть черным.\n",
    "Давайте посмотрим, как найти контуры двоичного изображения с помощью метода **Фридмана**:\n",
    "\n",
    "**Цепной код Фримена (Фридмана) (Freeman Chain Code)**\n",
    "\n",
    "Цепные коды применяются для представления границы в виде последовательности отрезков прямых линий определённой длины и направления. В основе этого представления лежит 4- или 8- связная решётка. Длина каждого отрезка определяется разрешением решётки, а направления задаются выбранным кодом.\n",
    "(для представления всех направлений в 4-связной решётке достаточно 2-х бит, а для 8-связной решётки цепного кода требуется 3 бита)\n",
    "\n",
    "<img src=\"https://i.ibb.co/6tyGLKS/freeman_chain_code.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "Если честно, то у меня ни разу ни получилось применить контурный анализ в реальных задачах. Уж слишком идеальные условия требуются. То граница не найдётся, то шумов слишком много. Но, если нужно что-то распознавать в идеальных условиях $-$ то контурный анализ замечательный вариант. Очень быстро работает, красивая математика и понятная логика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "laplac = cv2.Laplacian(gray_img, cv2.THRESH_BINARY, scale=0.15, ksize=5)\n",
    "laplac = cv2.medianBlur(laplac, 3)\n",
    "im2, contours, hierarchy = cv2.findContours(laplac, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "plt.imshow(laplac, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрите, в функции **cv2.findContours()** есть три аргумента, первый - исходное изображение, второй - режим поиска контура, третий - метод аппроксимации контура. И это выводит измененное изображение, контуры и иерархию. contours - это список всех контуров в изображении на языке Python. Каждый отдельный контур представляет собой массив Numpy с координатами (x, y) граничных точек объекта.\n",
    "\n",
    "*Мы обсудим второй и третий аргументы, и подробнее об иерархии позже.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как нарисовать контуры?\n",
    "Для рисования контуров используется функция **cv2.drawContours()**. Его также можно использовать для рисования любой фигуры, если у вас есть граничные точки. Его первый аргумент является исходным изображением, второй аргумент - это контуры, которые должны быть переданы в виде списка Python, третий аргумент - это индекс контуров (полезно при рисовании отдельного контура. Чтобы нарисовать все контуры, передайте -1), а остальные аргументы - это цвет, толщина и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## нарисуем все найденные контуры\n",
    "img1 = gray_img.copy()\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img1, contours, -1, (255,0,0), 3)\n",
    "\n",
    "plt.imshow(img1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## нарисуем один выбранный контур\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img2, contours, 53, (255,0,0), 10)\n",
    "\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Features\n",
    "\n",
    "Пройдемся по основным методам работы с характеристиками конутра, которые доступны в OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Моменты\n",
    "Моменты изображения помогают вам рассчитать некоторые функции, такие как центр масс объекта, площадь объекта и т. д.\n",
    "\n",
    "Функция **cv2.moments()** предоставляет словарь всех вычисленных значений моментов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Центральные моменты__\n",
    "\n",
    "$\\mu _{{pq}}=\\sum _{{x}}\\sum _{{y}}(x-{\\bar  {x}})^{p}(y-{\\bar  {y}})^{q}f(x,y)$\n",
    "\n",
    "__Масштабные инварианты__\n",
    "\n",
    "Инварианты $η_{ij}$ относительно сдвига и масштаба могут быть построены из центральных моментов путем деления на правильно масштабированный нулевой центральный момент:\n",
    "\n",
    "${\\displaystyle \\eta _{ij}={\\frac {\\mu _{ij}}{\\mu _{00}^{\\left(1+{\\frac {i+j}{2}}\\right)}}}\\}\\$, \n",
    "где i + j ≥ 2. Обратите внимание, что трансляционная инвариантность непосредственно следует только за счет использования центральных моментов.\n",
    "\n",
    "__Вращающиеся инварианты__\n",
    "\n",
    "Как показано в работе Ху, могут быть построены инварианты относительно перемещения, масштаба и вращения:\n",
    "\n",
    "${\\displaystyle I_{1}=\\eta _{20}+\\eta _{02}} I_{1}=\\eta _{{20}}+\\eta _{{02}}$\n",
    "\n",
    "${\\displaystyle I_{2}=(\\eta _{20}-\\eta _{02})^{2}+4\\eta _{11}^{2}} I_{2}=(\\eta _{{20}}-\\eta _{{02}})^{2}+4\\eta _{{11}}^{2}$\n",
    "\n",
    "${\\displaystyle I_{3}=(\\eta _{30}-3\\eta _{12})^{2}+(3\\eta _{21}-\\eta _{03})^{2}} I_{3}=(\\eta _{{30}}-3\\eta _{{12}})^{2}+(3\\eta _{{21}}-\\eta _{{03}})^{2}$\n",
    "\n",
    "${\\displaystyle I_{4}=(\\eta _{30}+\\eta _{12})^{2}+(\\eta _{21}+\\eta _{03})^{2}} I_{4}=(\\eta _{{30}}+\\eta _{{12}})^{2}+(\\eta _{{21}}+\\eta _{{03}})^{2}$\n",
    "\n",
    "${\\displaystyle I_{5}=(\\eta _{30}-3\\eta _{12})(\\eta _{30}+\\eta _{12})[(\\eta _{30}+\\eta _{12})^{2}-3(\\eta _{21}+\\eta _{03})^{2}]+(3\\eta _{21}-\\eta _{03})(\\eta _{21}+\\eta _{03})[3(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}]}$\n",
    "\n",
    "${\\displaystyle I_{6}=(\\eta _{20}-\\eta _{02})[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}]+4\\eta _{11}(\\eta _{30}+\\eta _{12})(\\eta _{21}+\\eta _{03})}$\n",
    "\n",
    "${\\displaystyle I_{7}=(3\\eta _{21}-\\eta _{03})(\\eta _{30}+\\eta _{12})[(\\eta _{30}+\\eta _{12})^{2}-3(\\eta _{21}+\\eta _{03})^{2}]-(\\eta _{30}-3\\eta _{12})(\\eta _{21}+\\eta _{03})[3(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{21}+\\eta _{03})^{2}].}$\n",
    "\n",
    "${\\displaystyle I_{8}=\\eta _{11}[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{03}+\\eta _{21})^{2}]-(\\eta _{20}-\\eta _{02})(\\eta _{30}+\\eta _{12})(\\eta _{03}+\\eta _{21})}$\n",
    "\n",
    "${\\displaystyle I_{8}=\\eta _{11}[(\\eta _{30}+\\eta _{12})^{2}-(\\eta _{03}+\\eta _{21})^{2}]-(\\eta _{20}-\\eta _{02})(\\eta _{30}+\\eta _{12})(\\eta _{03}+\\eta _{21})}$\n",
    "\n",
    "Они хорошо известны как инварианты моментов Ху.\n",
    "\n",
    "Первый, I1, аналогичен моменту инерции вокруг центроида изображения, где интенсивности пикселей аналогичны физической плотности. Последний, I7, является косоинвариантным, что позволяет ему отличать зеркальные изображения от других идентичных изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('img/RGB_cube.png')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "laplac = cv2.Laplacian(gray_img, cv2.THRESH_BINARY, scale=1, ksize=5)\n",
    "laplac = cv2.medianBlur(laplac, 3)\n",
    "im2, contours, hierarchy = cv2.findContours(laplac, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "cnt = contours[100]\n",
    "M = cv2.moments(cnt)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Коэффициент асимметрии\n",
    "import math\n",
    "sigma_x = math.sqrt(M['m20']/M['m00'])\n",
    "sigma_y = math.sqrt(M['m02']/M['m00'])\n",
    "\n",
    "k_x = M['m30']/sigma_x**3\n",
    "k_y = M['m03']/sigma_y**3\n",
    "\n",
    "print(k_x, k_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этих моментов вы можете извлечь полезные данные, такие как площадь, центроид и т.д. Центроид определяется отношениями, ${C_x = \\frac{M_{10}}{M_{00}}}$ and ${C_y = \\frac{M_{01}}{M_{00}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cx = int(M['m10']/M['m00'])\n",
    "Cy = int(M['m01']/M['m00'])\n",
    "print('Cx =', Cx, 'Cy =', Cy)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cnt = contours[100]\n",
    "cv2.drawContours(img2, [cnt], 0, (255,0,0), 3)\n",
    "plt.scatter(Cx, Cy, color='blue')\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Пример сохранения моментов__\n",
    "\n",
    "<img src=\"https://i.ibb.co/1nL7q4w/HuMoments.png\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Расстояние между двумя фигурами с помощью matchShapes\n",
    "\n",
    "В этом разделе мы узнаем, как использовать моменты Ху, чтобы найти расстояние между двумя фигурами. Если расстояние маленькое, формы близки по внешнему виду, а если расстояние большое, то фигуры находятся дальше друг от друга по внешнему виду.\n",
    "\n",
    "OpenCV предоставляет простую в использовании служебную функцию matchShapes, которая берет два изображения (или контура) и находит расстояние между ними с помощью Hu Moments. Таким образом, вам не нужно явно вычислять моменты Ху. Просто оцифруйте изображения и используйте matchShapes.\n",
    "\n",
    "Использование показано ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('img/lk.jpg')\n",
    "## для отрисовки в pyplot\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "rows, cols = img.shape\n",
    "\n",
    "M1 = cv2.getRotationMatrix2D((cols/2, rows/2), 25, scale=1.0)\n",
    "M2 = cv2.getRotationMatrix2D((300, 700), -15, scale=0.75)\n",
    "M3 = cv2.getRotationMatrix2D((300, 100), 45, scale=2.0)\n",
    "\n",
    "## визуализация\n",
    "fig, m_axs = plt.subplots(1, 2, figsize=(20,8))\n",
    "ax1, ax2 = m_axs\n",
    "\n",
    "dst1 = cv2.warpAffine(img.copy(), M1, (cols, rows))\n",
    "ax1.imshow(dst1)\n",
    "ax1.grid()\n",
    "ax1.set_title('M1 преоразование', fontsize=15)\n",
    "dst2 = cv2.warpAffine(img.copy(), M2, (cols, rows))\n",
    "ax2.imshow(dst2)\n",
    "ax2.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = cv2.matchShapes(dst1,dst2,cv2.CONTOURS_MATCH_I1,0)\n",
    "d2 = cv2.matchShapes(dst1,dst2,cv2.CONTOURS_MATCH_I2,0)\n",
    "d3 = cv2.matchShapes(dst1,dst2,cv2.CONTOURS_MATCH_I3,0)\n",
    "\n",
    "print(d1, d2, d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что есть три вида расстояний, которые можно использовать с помощью третьего параметра (CONTOURS_MATCH_I1, CONTOURS_MATCH_I2 или CONTOURS_MATCH_I3).\n",
    "\n",
    "Два изображения (im1 и im2) похожи, если указанные выше расстояния малы. Вы можете использовать любую меру расстояния. Они обычно дают похожие результаты.\n",
    "\n",
    "Давайте посмотрим, как определяются эти три расстояния.\n",
    "\n",
    "Пусть $ D (A, B) $ - расстояние между формами $ A $ и $ B $, а $ H ^ A_i $ и $ H ^ B_i $ - логарифмические преобразования $ i ^ {th} $ Ху Моментов для фигур $ A $ и $ B $. Расстояния, соответствующие трем случаям, определяются как\n",
    "\n",
    "CONTOURS_MATCH_I1\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\left | \\frac{1}{H^B_i} - \\frac{1}{H^A_i} \\right |  \\end{align*}\n",
    "\n",
    "CONTOURS_MATCH_I2\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\left | H^B_i - H^A_i \\right |  \\end{align*}\n",
    "\n",
    "CONTOURS_MATCH_I3\n",
    "   \\begin{align*} D(A, B) = \\sum^{6}_{i=0} \\frac{\\left | H^A_i - H^B_i \\right |}{\\left | H^A_i \\right |}  \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Custom distance measure\n",
    "Если вы хотите определить собственную меру расстояния между двумя фигурами, вы можете легко это сделать. Например, вы можете использовать евклидово расстояние между моментами Ху, заданными\n",
    "\n",
    "  \\begin{align*} D(A, B) = \\sqrt { \\sum^{6}_{i=0} \\left ( H^B_i - H^A_i \\right )^2 } \\end{align*}\n",
    "\n",
    "Сначала вы вычисляете трансформированные в журнал моменты Ху, как упомянуто в предыдущем разделе, а затем сами вычисляете расстояние вместо использования matchShapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Контурная зона\n",
    "Площадь контура задается функцией **cv2.contourArea()** или из моментов, **M['m00']**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Периметр контура\n",
    "Это также называется длиной дуги. Это можно узнать с помощью функции **cv2.arcLength()**. Второй аргумент указывает, является ли фигура замкнутым контуром (если передан True) или просто кривой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimeter = cv2.arcLength(cnt,True)\n",
    "print(round(perimeter, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Контурное приближение\n",
    "Он приближает форму контура к другой форме с меньшим количеством вершин в зависимости от заданной нами точности. Это реализация алгоритма Дугласа-Пекера. Проверьте страницу википедии на алгоритм и демонстрацию.\n",
    "\n",
    "Чтобы понять это, предположим, что вы пытаетесь найти квадрат на изображении, но из-за некоторых проблем на изображении вы получили не идеальный квадрат, а \"плохую форму\" (как показано на первом изображении ниже). Теперь вы можете использовать эту функцию для аппроксимации формы. В этом случае второй аргумент называется эпсилон, который является максимальным расстоянием от контура до приближенного контура. Это параметр точности. Для правильного вывода необходим мудрый выбор эпсилона.\n",
    "\n",
    "<img src=\"https://i.ibb.co/jHvc2HS/approx.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "Выше, на втором изображении, зеленая линия показывает приблизительную кривую для эпсилона = $10$% длины дуги. Третье изображение показывает то же самое для эпсилона = $1$% длины дуги. Третий аргумент указывает, является ли кривая замкнутой или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1*cv2.arcLength(cnt,True)\n",
    "approx = cv2.approxPolyDP(cnt,epsilon,True)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawContours(img2, [approx], 0, (255,0,0), 3)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Выпуклый контур\n",
    "Выпуклая оболочка будет похожа на контурную аппроксимацию, но это не так (оба могут давать одинаковые результаты в некоторых случаях). Здесь функция **cv2.convexHull()** проверяет кривую на наличие дефектов выпуклости и исправляет ее. Вообще говоря, выпуклые кривые $-$ это кривые, которые всегда выпуклые или, по крайней мере, плоские. И если он выпуклый внутри, это называется дефектами выпуклости. Например, проверьте изображение ниже. Красная линия показывает выпуклый корпус руки. Двусторонние стрелки показывают дефекты выпуклости, которые представляют собой локальные максимальные отклонения корпуса от контуров.\n",
    "<img src=\"https://i.ibb.co/Z2nPDCM/convexitydefects.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "**hull = cv2.convexHull(points, clockwise, returnPoints)**\n",
    "\n",
    "* **points** $-$ точки контура.\n",
    "\n",
    "* **clockwise** $-$ флаг ориентации. Если это правда, выходной выпуклый корпус ориентирован по часовой стрелке. В противном случае он ориентирован против часовой стрелки.\n",
    "\n",
    "* **returnPoints** $-$ по умолчанию True. Затем он возвращает координаты точек корпуса. Если False, он возвращает индексы точек контура, соответствующие точкам корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull = cv2.convexHull(cnt)\n",
    "hull_id = cv2.convexHull(cnt, returnPoints=False)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но если вы хотите найти дефекты выпуклости, вам нужно передать returnPoints = False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Проверка выпуклости\n",
    "Есть функция, чтобы проверить, является ли кривая выпуклой или нет, **cv2.isContourConvex()**. Это просто возвращает True или False\n",
    "\n",
    "Подумайте, как это можно сделать без этой функции?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = cv2.isContourConvex(cnt)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ограничивающий прямоугольник\n",
    "\n",
    "#### 7.a. Прямой ограничивающий прямоугольник\n",
    "Это прямой прямоугольник, он не учитывает вращение объекта. Таким образом, площадь ограничивающего прямоугольника не будет минимальной. Он находится функцией **cv2.boundingRect()**.\n",
    "\n",
    "Пусть $(x,y)$ $-$ верхняя левая координата прямоугольника, а $(w,h)$ $-$ его ширина и высота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img2,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.b. Повернутый прямоугольник\n",
    "Здесь ограничивающий прямоугольник рисуется с минимальной площадью, поэтому он учитывает и вращение. Используемая функция $-$ **cv2.minAreaRect()**. Он возвращает структуру **Box2D**, которая содержит следующие детали $-$ (центр $(x, y)$, (ширина, высота), угол поворота). Но чтобы нарисовать этот прямоугольник, нам нужно $4$ угла прямоугольника. Получается функцией **cv2.boxPoints()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "\n",
    "rect = cv2.minAreaRect(cnt)\n",
    "box = cv2.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "cv2.drawContours(img2,[box],0,(0,0,255),2)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Подгонка линии\n",
    "Точно так же мы можем подогнать линию к набору точек. Ниже изображение содержит набор белых точек. Мы можем приблизить к нему прямую линию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = gray_img.copy()\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "rows,cols = img.shape[:2]\n",
    "[vx,vy,x,y] = cv2.fitLine(cnt, cv2.DIST_L2,0,0.01,0.01)\n",
    "lefty = int((-x*vy/vx) + y)\n",
    "righty = int(((cols-x)*vy/vx)+y)\n",
    "cv2.line(img2,(cols-1,righty),(0,lefty),(0,255,0),2)\n",
    "\n",
    "plt.grid()\n",
    "plt.imshow(img2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Соотношение сторон\n",
    "Это отношение ширины к высоте ограничивающего прямоугольника объекта.\n",
    "\n",
    "${AspectRatio = \\frac{Width}{Height}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "aspect_ratio = float(w)/h\n",
    "\n",
    "print(aspect_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Степень\n",
    "Степень $-$ это отношение площади контура к площади ограничивающего прямоугольника.\n",
    "\n",
    "${Extent=\\frac{Object\\ Area}{Bounding\\ Rectangle\\ Area}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "rect_area = w*h\n",
    "extent = float(area)/rect_area\n",
    "\n",
    "print(extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solidity\n",
    "Solidity $-$ это отношение площади контура к его площади выпуклой оболочки.\n",
    "\n",
    "$Solidity = \\frac{Contour \\ Area}{Convex \\ Hull \\ Area}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "hull = cv2.convexHull(cnt)\n",
    "hull_area = cv2.contourArea(hull)\n",
    "solidity = float(area)/hull_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Эквивалентный диаметр\n",
    "Эквивалентный диаметр $-$ это диаметр круга, площадь которого равна площади контура.\n",
    "\n",
    "$Equivalent \\ Diameter = \\sqrt{\\frac{4 \\cdot \\ Contour \\ Area}{\\pi}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "equi_diameter = np.sqrt(4*area/np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ориентация\n",
    "Ориентация $-$ это угол, под которым направлен объект. Следующий метод также дает длины **Major Axis** и **Minor Axis**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y),(MA,ma),angle = cv2.fitEllipse(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Маска и пиксельные точки\n",
    "В некоторых случаях нам могут понадобиться все точки, которые составляют этот объект. Это можно сделать следующим образом:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(gray_img.shape,np.uint8)\n",
    "cv2.drawContours(mask,[cnt],0,255,-1)\n",
    "pixelpoints = np.transpose(np.nonzero(mask))\n",
    "#pixelpoints = cv2.findNonZero(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь два метода, один из которых использует функции Numpy, а другой $-$ функцию OpenCV (последняя закомментированная строка), дают то же самое. Результаты тоже такие же, но с небольшой разницей. Numpy дает координаты в формате **(строка, столбец)**, а OpenCV - в формате **(x, y)**. Так что в основном ответы будут взаимозаменяемы. Обратите внимание, что row = x и column = y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Максимальное значение, минимальное значение и их местоположение\n",
    "Мы можем найти эти параметры, используя изображение маски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(gray_img,mask = mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Средний цвет или средняя интенсивность\n",
    "Здесь мы можем найти средний цвет объекта. Или это может быть средняя интенсивность объекта в режиме градаций серого. Мы снова используем ту же маску, чтобы сделать это.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = cv2.mean(gray_img, mask = mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Экстремальные точки\n",
    "Экстремальные точки означают самые верхние, самые нижние, самые правые и самые левые точки объекта.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftmost = tuple(cnt[cnt[:,:,0].argmin()][0])\n",
    "rightmost = tuple(cnt[cnt[:,:,0].argmax()][0])\n",
    "topmost = tuple(cnt[cnt[:,:,1].argmin()][0])\n",
    "bottommost = tuple(cnt[cnt[:,:,1].argmax()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leftmost, rightmost,'\\n', topmost, bottommost)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.scatter(leftmost[0], leftmost[1])\n",
    "plt.scatter(rightmost[0], rightmost[1])\n",
    "plt.scatter(topmost[0], topmost[1])\n",
    "plt.scatter(bottommost[0], bottommost[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразования Хафа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория\n",
    "**Hough Transform** $-$ популярный метод обнаружения любой формы, если вы можете представить эту форму в математической форме. Он может обнаружить форму, даже если он немного сломан или искажен. Посмотрим, как это работает для линии.\n",
    "\n",
    "Линия может быть представлена как ${y = mx + c}$ или в параметрической форме, как ${\\rho=x\\cos(\\theta)+y\\sin(\\theta)}$, где ${\\rho}$ $-$ перпендикулярное расстояние от начала координат до линии, а $\\theta$ $-$ угол, образованный этой перпендикулярной линией и горизонтальной осью, измеренный в счетчике по часовой стрелке (это направление зависит от того, как вы представляете систему координат. Это представление используется в OpenCV). Проверьте изображение ниже:\n",
    "\n",
    "IMG\n",
    "\n",
    "Таким образом, если линия проходит ниже начала координат, она будет иметь положительное значение $\\rho$ и $\\theta <180$. Если она идет выше начала координат, вместо того, чтобы брать угол больше $180$, угол выбирается меньше $180$, а значение ${\\rho}$ принимается отрицательным. Любая вертикальная линия будет иметь $0$ градусов, а горизонтальные линии будут иметь $90$ градусов.\n",
    "\n",
    "Теперь посмотрим, как работает Hough Transform для линий. Любая линия может быть представлена в этих двух терминах ${(\\rho, \\theta)}$. Поэтому сначала он создает двумерный массив или аккумулятор (для хранения значений двух параметров), и для него изначально устанавливается значение ${\\theta}$. Пусть строки обозначают ${\\rho}$, а столбцы $-$ ${\\theta}$. Размер массива зависит от точности, которая вам нужна. Предположим, вы хотите, чтобы точность углов составляла 1 градус, вам нужно $180$ столбцов. Для ${\\rho}$ максимально возможное расстояние $-$ это диагональная длина изображения. Таким образом, с точностью до одного пикселя, количество строк может быть диагональной длины изображения.\n",
    "\n",
    "Рассмотрим изображение размером $100\\times100$ с горизонтальной линией посередине. Возьмите первую точку линии. Вы знаете его $(x,y)$ значения. Теперь в линейном уравнении поместите значения ${\\theta = 0,1,2, \\dots, 180}$ и проверьте полученное значение ${\\rho}$. Для каждой ${(\\rho, \\theta)}$ пары вы увеличиваете значение на единицу в нашем аккумуляторе в соответствующих ${(\\rho, \\theta)}$ ячейках. Так что теперь в аккумуляторе ячейка $(50,90) = 1$ вместе с некоторыми другими ячейками.\n",
    "\n",
    "Теперь возьмите вторую точку на линии. Сделайте так же, как указано выше. Увеличьте значения в ячейках, соответствующих ${(\\rho, \\theta)}$, которые вы получили. На этот раз ячейка $(50,90) = 2$. На самом деле вы голосуете за значения ${(\\rho, \\theta)}$. Вы продолжаете этот процесс для каждой точки на линии. В каждой точке ячейка $(50,90)$ будет увеличена или оценена, в то время как другие ячейки могут или не могут быть проголосованы. Таким образом, в конце ячейка $(50,90)$ получит максимальное количество голосов. Поэтому, если вы будете искать в аккумуляторе максимальное количество голосов, вы получите значение $(50,90)$, которое говорит, что на этом изображении есть линия на расстоянии $50$ от начала координат и под углом $90$ градусов. Это хорошо показано на анимации ниже:\n",
    "\n",
    "<img src=\"https://i.ibb.co/rGMQxwH/houghlinesdemo.gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***\n",
    " Более подробное описание алгоритма по [ссылке]( http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование Хафа в OpenCV\n",
    "Все объясненное выше инкапсулировано в функцию OpenCV, **cv2.HoughLines(image,rho,theta,threshold,args*)**. Он просто возвращает массив значений ${(\\rho, \\theta)}$. ${\\rho}$ измеряется в пикселях, а ${\\theta}$ измеряется в радианах.\n",
    "\n",
    "* **image** $-$ «входное изображение», должен быть двоичным изображением, поэтому примените пороговое значение или используйте обнаружение контуров, прежде чем находить применение грубого преобразования\n",
    "* **rho** $-$ разрешение по расстоянию аккумулятора в пикселях\n",
    "* **theta** $-$ угловое разрешение аккумулятора в радианах.\n",
    "* **threshold** $-$ это порог, который означает минимальное количество голосов, которое он должен получить, чтобы его считали линией. Помните, что количество голосов зависит от количества точек на линии. Таким образом, он представляет минимальную длину линии, которая должна быть обнаружена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T20:32:22.959246Z",
     "start_time": "2019-07-16T20:32:22.363762Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/road_2.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 350, 450, apertureSize=3)\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=150, maxLineGap=10)\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "    \n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вероятностное преобразование Хафа\n",
    "В грубом преобразовании вы можете видеть, что даже для строки с двумя аргументами требуется много вычислений. Вероятностное преобразование Хафа - это оптимизация преобразования Хафа, которое мы видели. Он не учитывает все точки, а только случайное подмножество точек, и этого достаточно для обнаружения линии. Просто мы должны уменьшить порог. Смотрите изображение ниже, которое сравнивает Hough Transform и Вероятностное Hough Transform в пространстве Hough.\n",
    "\n",
    "Реализация в OpenCV  **cv2.HoughLinesP(image,rho,theta,threshold,minLineLength,maxLineGap)**. У него есть два новых аргумента.\n",
    "\n",
    "* **minLineLength** $-$ минимальная длина линии. Сегменты линии короче этого отклоняются.\n",
    "* **maxLineGap** $-$ максимально допустимый разрыв между отрезками, чтобы рассматривать их как одну линию.\n",
    "\n",
    "Лучше всего то, что он напрямую возвращает две конечные точки линий. В предыдущем случае вы получили только параметры линий, и вам нужно было найти все точки. Здесь все прямое и простое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T20:32:27.493643Z",
     "start_time": "2019-07-16T20:32:26.945169Z"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('img/road_2.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 350, 450, apertureSize=3)\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi/180, 100, minLineLength=150, maxLineGap=10)\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "    \n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашняя работа №4\n",
    "\n",
    "[Форма]() для отправки решения.\n",
    "\n",
    "__ДЕДЛАЙНЫ__:\n",
    "\n",
    "Задача №1 - 23 июля 23:59\n",
    "\n",
    "Задача №2 - 24 июля 23:59\n",
    "\n",
    "Укажите в названии файла: \n",
    "1. имя и фамилия\n",
    "2. номер домашки\n",
    "3. номер задания\n",
    "\n",
    "(NAME_LES_TASK.ipynb)\n",
    "\n",
    "#### Задача №1 - Можете ли вы отличить сорняки от рассады?\n",
    "\n",
    "Теперь приступим к задаче классификации на картинках. Реализуйте программу, которая определяет тип рассады на изображении. \n",
    "\n",
    "Для того, чтобы определить характерные особенности каждого типа рассады, у вас есть train. Train это папка, в которой картинки уже классифицированы и лежат в соответствующих папках. Исходя из этой информации можете найти признаки, присущие конкретному растению.\n",
    "\n",
    "Проверка вашего решения будет на происходить на test. В папке test уже нет метки класса для каждой картинки. \n",
    "\n",
    "[Ссылка на Яндекс-диск](https://yadi.sk/d/0Zzp0klXT0iRmA), все картинки тут.\n",
    "\n",
    "Примеры изображений для теста:\n",
    "<table><tr>\n",
    "    <td> <img src=\"https://i.ibb.co/tbqR37m/fhj.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "    <td> <img src=\"https://i.ibb.co/6yL3Wmt/sfg.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "    <td> <img src=\"https://i.ibb.co/pvn7NvF/asd.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача №2 - Ложки, сахар\n",
    "\n",
    "Теперь попробуем решить задачу сегментации, т.е. теперь нужно найти маску определенного класса объекта на изображении. Ваша задача - определить классы предметов на изображении. Известно, что класса всего 4: чайная ложка, монета, ручка, катушка ниток.\n",
    "\n",
    "Программа должна вывести исходное изображние, где каждый класс предмета помещен в цветную рамку. Каждый класс должен иметь свой цвет.\n",
    "\n",
    "Примеры изображений:\n",
    "<table><tr>\n",
    "    <td> <img src=\"https://i.ibb.co/cvM76FT/ex_1.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "    <td> <img src=\"https://i.ibb.co/LdWL9dp/ex_5.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "</tr></table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
